% TyDe wants sigplan & ``Extended abstract'' clearly in the title.
% HOPE doesn't seem to care.
% FHPC wants acmart, doesn't specify sigplan.

% https://icfp18.sigplan.org/track/tyde-2018#Call-for-Contributions
% https://icfp18.sigplan.org/track/hope-2018-papers#Call-for-Presentations
% https://icfp18.sigplan.org/track/FHPC-2018-papers#FHPC-2018-Call-for-Papers

\documentclass[sigplan,screen,review,timestamp,dvipsnames]{acmart}
%\usepackage[scaled=1.023]{cochineal}
%\usepackage[scaled=1.04]{cochineal}\linespread{1.03}
\setlength\arraycolsep{2.5pt}


%% ---- Packages ----
\usepackage{amsmath,latexsym,stmaryrd}
\usepackage[nameinlink]{cleveref}
\usepackage{mathtools}          % \dblcolon
\usepackage{anyfontsize}
\usepackage{lipsum}
\usepackage{listings}
\lstset{language=prolog, columns=fullflexible,
  basicstyle=\ttfamily, commentstyle=\color{Green}}

%% %% Avoid ``too many math alphabets'' error.
%% \newcommand\hmmax{0}
%% \newcommand\bmmax{0}
%% \usepackage{bm}


%% ---- Top matter ----
\title{Finding fixed points faster}
% The derivative of a fixed point is the fixed point of its derivative
% Finding fixed points faster
% Finding fixed points faster by differentiation
% Deriving incremental recursive queries
\author{Michael Arntzenius}
\affiliation{University of Birmingham}
\email{daekharel@gmail.com}
\setcopyright{none}
\settopmatter{printacmref=false}

%% TODO
%\terms{blah}
%\keywords{blah}


%% ---- Commands ----
\newcommand{\todo}[1]{{\color{ACMPurple}#1}}
\newcommand{\hilited}{\color{Cyan}}

\newcommand{\naive}{na\"ive}

\newcommand{\D}{\Delta}
\newcommand{\bnfeq}{\dblcolon=}
\newcommand{\bnfcont}{}
\newcommand{\pipe}{~|~}
\newcommand{\x}{\times}
\newcommand{\fn}{\lambda}
\newcommand{\binder}{.~}
\newcommand{\bind}[1]{#1\binder}
\newcommand{\fnof}[1]{\fn\bind{#1}}
\newcommand{\setfor}[2]{\{#1 ~|~ #2\}}

\renewcommand{\d}{\delta}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\kw}[1]{\textsf{\bfseries #1}}
\newcommand{\tlv}[1]{\textsf{#1}}
\newcommand{\var}[1]{\mathit{#1}}
\newcommand{\dee}[1]{\var{d#1}}
\newcommand{\subst}[1]{[#1]\,}

\newcommand{\mto}{\overset{+}{\to}}
\renewcommand{\mto}{\Rightarrow}
\newcommand{\tset}[1]{\{#1\}}
\newcommand{\tbool}{\textsf{bool}}

\newcommand{\eset}[1]{\{#1\}}
\newcommand{\ewhen}[1]{\kw{when}\,(#1)~}
\newcommand{\eif}[2]{\kw{if}~#1~\kw{then}~#2~\kw{else}~}
\newcommand{\efor}[1]{\bigcup(#1)~}
\newcommand{\elet}[1]{\kw{let}~#1~\kw{in}~}
\newcommand{\efix}{\kw{fix}~}



\begin{document}

%% TODO: include if enough space.

%% \begin{abstract}
%%   We sketch a generalisation of the classic Datalog \emph{semi\naive{}
%%     evaluation} optimisation to the higher-order functional language Datafun.
%% \end{abstract}

\maketitle

\section{Introduction}

%% Logic programming has a powerful appeal: declare what counts as a solution, and
%% let the computer find it! Unfortunately, traditional logic programming systems
%% like Prolog have trouble going truly \emph{higher-order} in the way functional
%% programmers are used to. Seeking to have their cake and eat it, too, functional
%% programmers have learned to emulate logic programming using the effect of
%% \emph{nondeterminism}, usually implemented as backtracking.

%% However, backtracking search is often inefficient; logic programmers have
%% explored other useful strategies. Datalog~\citep{datalog} takes an extreme
%% approach, allowing only predicates with finite extent. This allows bottom-up
%% evaluation, which easily handles queries (such as transitive closure) which are
%% difficult or inefficient to implement in Prolog.

Functional programmers have learned to emulate logic programming using the
effect of \emph{nondeterminism}, usually implemented as backtracking. However,
backtracking search is often inefficient; logic programmers have explored other
useful strategies.
%
Datalog~\citep{datalog} takes an extreme approach, allowing only predicates with
finite extent. This allows bottom-up evaluation, which easily handles queries
(such as transitive closure) that are inefficient to solve by brute-force
search.

Datafun~\cite{datafun} shows that higher-order functional programs can emulate
Datalog using a \emph{bottom-up nondeterminism effect} (a \emph{finite set
  monad}) combined with \emph{monotone fixed points}. Here, we sketch the
translation to Datafun of a classic Datalog optimisation, semi\naive{}
evaluation, which avoids needlessly re-deducing facts when evaluating a
recursive predicate.

%% TODO: include if space permits.
%% TODO: citation for Datalog folklore

%% Datalog folklore suggests semi\naive{} evaluation can be understood in terms of
%% \emph{derivatives}; we substantiate this by applying recent work by
%% \citet{incremental} on \todo{TODO: Cai et al stuff}.


\section{Datalog, \naive{}ly and semi\naive{}ly}\label{sec:datalog}

This simple Datalog program computes reachability in a graph, given its
\texttt{edge} relation:
%
\begin{lstlisting}
path(X,Y) :- edge(X,Y).
path(X,Z) :- edge(X,Y), path(Y,Z).
\end{lstlisting}

A path is either an edge, or an edge followed by a path. But how does Datalog
find these paths? Let's identify a predicate with the set of argument-tuples it
holds of. Then we can compute \texttt{path} as the least fixed point of this
function:
%
\[
\begin{array}{l}
  \tlv{step}~ \var{path} = \setfor{(x,y)}{(x,y) \in \tlv{edge}}\\
  \phantom{\tlv{step}~ \var{path}} \hspace{.5pt}\cup
  \setfor{(x,z)}{(x,y) \in \tlv{edge}, (y,z) \in \var{path}}
\end{array}
\]

The \naive{} approach is to iterate the \tlv{step} function, computing the
sequence $\emptyset, \tlv{step}^1(\emptyset), \tlv{step}^2(\emptyset), ...$
until it reaches a fixed point $\tlv{step}^k(\emptyset) =
\tlv{step}^{k+1}(\emptyset)$.
%
This works, but observe that $\tlv{step}^i(\emptyset) \subseteq
\tlv{step}^{i+1}(\emptyset)$. This means we are doing \emph{redundant
  computation} --- if step $i$ appends an edge $(x,y)$ to a path $(y,z)$ to
discover the path $(x,z)$, step $i+1$ re-discovers it the same way. We really
want to compute the \emph{change} between iterations:
%
\[
\begin{array}{rcl}
  \tlv{path} &=& \tlv{iterate} \;\emptyset \;\tlv{edge}\\
  \tlv{iterate} \;x \;\dee x &=&
  \eif{\dee x \subseteq x}{x}\\
  && \tlv{loop} \;(x \cup \dee x) \;(\delta\tlv{step} \;dx)\\
  \delta\tlv{step} \;\var{dpaths} &=&
  \setfor{(x,z)}{(x,y) \in \tlv{edge}, (y,z) \in \var{dpaths}}\\
\end{array}
\]

This insight underlies the technique known in Datalog as \emph{semina\"ive
  evaluation}. \todo{TODO: Describe it. Maybe don't need to describe it in full,
  just suggest that it approximates the change between approximations by static
  transformation.}


\section{Datafun, na\"ively}

\begin{figure}
  %% TODO: do we need types?
  \[
  \begin{array}{rcl}
    A,B &\bnfeq& \tbool \pipe \tset{A} \pipe A \to B \pipe A \mto B
    \\
    e &\bnfeq& \fnof{x} e \pipe e_1\:e_2 \pipe \eset{\vec{e}} \pipe e_1 \cup e_2
    \pipe \efor{x \in e_1}~ e_2
    \\ &\bnfcont&  \efix e \pipe \ewhen{e_1} e_2 \pipe \eif{e_1}{e_2}{e_3}
    %% FIXME: need equality!
  \end{array}
  \]\vspace{-1em}
  \caption{A fragment of Datafun}
  \label{fig:datafun}
\end{figure}

You've actually already seen some Datafun code: the \tlv{step} function in
\cref{sec:datalog}! Datafun is a typed higher-order functional language equipped
with a \emph{finite set monad}, which supports set-comprehension syntax sugar in
the usual way~\cite{comprehending-monads}. Like Datalog, Datafun is
\emph{total}: all programs terminate.

\Cref{fig:datafun} gives the fragment of Datafun we consider here. For the full
language, see \citet{datafun}. We write monadic bind $\efor{x \in e_1} e_2$,
meaning ``the union of the sets $e_2$ for each $x \in e_1$''.
%
Datafun can also compute \emph{fixed points} of functions on finite sets, $\efix
f$.\footnote{The full language generalizes both monadic bind and fixed points to
  \emph{semilattice types}; for simplicity we here consider only finite sets.}

To ensure $\efix f$ terminates, $f$ must be (among other things) monotone ($x
\subseteq y$ implies $f\:x \subseteq f\:y$), so Datafun's type system tracks
monotonicity of functions and expressions.
%
The type $A \mto B$ represents monotone functions, a subtype of all functions $A
\to B$. The expression $\ewhen{e_1}{e_2}$ yields the set $e_2$ if $e_1$ is true,
and $\emptyset$ otherwise; unlike \kw{if}, this is always monotone in $e_1$.

As we've seen, Datalog programs can be expressed using a combination of set
operations and fixed points. For example, \texttt{path} is
$(\efix{\tlv{step}})$. However, the semi\naive{} evaluation transformation,
formulated on Datalog, does not handle higher-order functions. Can we lift this limitation?

\section{Derivatives for Datafun}

%% TODO: citation for folklore

Folklore suggests semi\naive{} evaluation uses the \emph{derivative} of Datalog
rules.
%
\citet{incremental} formulate \emph{incremental computation} of higher-order
programs in terms of derivatives.
%
We confirm and generalise folklore by applying this work to Datafun, discovering
a faster way to compute $\efix f$ using the derivative of $f$.

\Citeauthor{incremental}'s incremental $\lambda$-calculus


\todo{TODO: describe our version of change structures and derivatives}

We use a variant suggested by \citet{atkey-changes}: a change structure for a
type $A$ is a triple $(\D A, \oplus, \zero)$. $\D A$ is a type representing
\emph{changes} to values of type $A$.
%
$\oplus : A \to \Delta A \to A$ is a function

\begin{figure}
  \[\begin{array}{rcl}
    \d x &=& dx\\
    \d(\fnof x e) &=& \fnof x \fnof{\dee x} \d{e}\\
    \d(e_1~e_2) &=& \d{e_1}~e_2~\d{e_2}\\
    \d{\eset{\vec e}} &=& \emptyset\\
    \d(e_1 \cup e_2) &=& \d{e_1} \cup \d{e_2}\\
    \d(\efor{x \in e_1} e_2)
    &=& \efor{x \in \d{e_1}} e_2\\
    &\cup& \efor{x \in e_1 \cup \d{e_1}}
    %\elet{\dee x = \dzero x}
    \subst{\zero\, x / \dee x}
    \d{e_2}
    \\
    \d(\efix f) &=& \efix (\d\!f~(\efix f))\\
    \d(\eif{e}{e_1}{e_2}) &=& \eif{e}{\d{e_1}}{\d{e_2}}\\
    \d(\ewhen{e_1}{e_2})
    &=& \eif{e_1}{\d{e_2}}\\
    && \ewhen{\d{e_1}} {e_2 \cup \d{e_2}}
  \end{array}\]
  \vspace{-.8em}
  \caption{Derivatives for a fragment of Datafun}
  \label{fig:derivatives}
\end{figure}

\begin{figure}
  \[
  \begin{array}{rcl}
  \efix f &=& \tlv{\naive}_f \,\emptyset \,(f\, \emptyset)\\
  \tlv{\naive}_f \,x \,\var{next}
  &=& \kw{if}~ x = \var{next} ~\kw{then}~ x ~\kw{else}\\
  && \tlv{\naive}_f \,\var{next} \,(f \,\var{next})

  \vspace{1em}\\

  \efix f &=& \tlv{semi\naive}_f \,\emptyset \,(f\, \emptyset)\\
  \tlv{semi\naive}_f \,x \,\dee x
  &=& \kw{if}~ \dee x \subseteq x ~\kw{then}~ x ~\kw{else}\\
  && \tlv{semi\naive}_f \,(x \cup \dee x) \,(\d\!f \,x \,\dee x)
  \end{array}
  \]
  \vspace{-.5em}
  \caption{Na\"ive and semina\"ive fixed point computation}
  \label{fig:defining-fix}
\end{figure}

\vspace{1em}

Observe that $\d(\efix f)$ must be a fixed point of $\d\!f\;(\efix f)$:
%% We discovered the definition of $\d(\efix f)$ in \cref{fig:derivatives} by
%% observing that, if it exists, $\d(\efix f)$ must be a fixed point of $\d\!f\;
%% (\efix f)$:
%
\begin{align*}
  {\hilited \d(\efix f)}
  &= \d(f\: (\efix f))
  & \text{expand fixed point}\\
  &= \d\!f\; (\efix f)\; {\hilited \d(\efix f)}
  & \text{rule for}~\delta(e_1\;e_2)
\end{align*}
%
Which suggests the following motto:
\begin{center}
  \large \scshape
  the derivative of a fixed point\nopagebreak\\
  is the fixed point of its derivative.
\end{center}
%
This is so beautiful it must be true. Nevertheless, we have proven the definition of $\delta(\efix f)$ in \cref{fig:derivatives} correct~\citep{fixderiv}.

%% TODO: code for naive-fix and seminaive-fix


%% Bibliography
\bibliographystyle{abbrvnat}
\bibliography{datafun}

\end{document}
