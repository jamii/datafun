* Sketch of paper
1. Lead with faster-fix and how it eg. speeds up transitive closure
   from O(n^2) to O(n).


* Theorems to put in paper
⟦-⟧ : Datafun → Poset is the standard semantics.
⦃-⦄ : Datafun → ΔPoset is the change semantics.
𝓤 : ΔPoset → Poset is the forgetful functor.
For A : ΔPoset, 𝓤A : Poset is the value poset
and ΔA : Poset is the change poset.

1. 𝓤⦃-⦄ = ⟦-⟧
2. Δ⦃A⦄ = ⟦ΔA⟧
3. The logical relations proof.


** OLD, broken version
TODO: fix up this theorem todo list
1. U⟦-⟧_Deriv = ⟦-⟧_Poset
2. □ ≅ ■
3. iso : ⟦A⟧_deriv ≅ ⟦A⟧_φ
4. Naturality, ⟦e⟧_deriv; iso = iso; ⟦e⟧_φ
5. Reading off ⟦-⟧_φ gives φ/δ translation, namely:
5a. ⟦φe⟧_Poset = U⟦e⟧_φ (= ⟦e⟧_Poset by Thm1).
5b. ⟦δe⟧_Poset is a derivative for (5a).

uh-oh, I think those last two don't even type-check!


* Roads not taken
There were a lot of choices we made that could have been made differently. I'd
like to have a section in the paper on them if there's room. In particular, we
could have, in no particular order:

1. Normalized first, then do seminaive transform only on first-order Datafun.

2. Used "change monoids" a la unpublished(?) Picallo & Michael PJ &c paper.

3. Tried to handle nested fixed points. This would involve 2nd and higher
   derivatives; could lazily compute them? Apply Paolo Giarrusso's thesis work?
   We didn't do this because it seems hard and Datalog can't do it.

4. Included the derivative transform, not just the seminaive transform.

5. Augmented our target language with dependent types (to specify "zero change
   for a value") and quotients to try to make Φ□A more closely model the
   semantics.

6. Used two _separate_ comonads, one for discreteness, one for preventing nested
   fixed points. But then we'd have to think about how they interact! Future
   work?

7. Only stored zero-changes for _functions_, not for everything inside a boxed
   type. This seems related to letting Φ(A_eq) = A_eq. Or perhaps having
   separate Δ□ and Δ.

8. We chose to let Δ□A = 1. It's possible letting Δ□A = □ΔA would also work,
   although I'm not sure. For example, then ΔΦ□A = Δ□(ΦA × ΔΦA) = □(ΔΦA × ΔΔΦA),
   which just feels icky. However, it does considerably simplify φ(split e) and
   δ(split e)! eg. δ(split e) = split δe.

9. The finite set type, {-}, can conceptually be separated into F□A, where F is
   the "free semilattice" functor. So why don't we do this - in our semantics at
   least, if not in our syntax?

   This would break our semantics into smaller components; for example, the
   requirement that given (f : Γ × □A → L) we can construct (f^* : Γ × {A} → L)
   can be satisfied by functoriality and tensorial strength of F and its
   initiality property as the free semilattice. It would also simplify our
   typing rules: the intro & elim rules for FA are the same as for {A} but
   monotone where {A} is discrete, neatly separating discreteness from
   "set"-ness.

   Unfortunately, it turns out this separation works in Poset, but not in
   ΔPoset. What is ΔFA? It's really not clear. Since Δ{A} = {A}, we want ΔF□A to
   be something like F□A, but there's no compositional way to do that. We could
   settle for ΔF□A = F□(A × ΔA) if we let ΔFA = F𝕍A and let ⊕ = ∨. But then what
   is the derivative of gen? The obvious one is gen' x dx = gen(x ⊕ dx). But ⊕
   is a *partial* function, so this doesn't work!

   Although it's possible there is some way to make this work, in my experience
   trying to separate {-} into F□ turned out to be a messy maze of dead-ends.

   OLD UN-UPDATED REASONS:

8. We have finite set types {-} in our syntax, but separate {A} into F□A in our
   semantics. Why not put F directly into our syntax? This simplifies our typing
   rules; it has the same intro & elim as {-} but without getting discreteness
   involved! However, this creates two complications:

   - First, then Φ{A} = ΦF□A = F□(ΦA × ΔΦA). So now our sets carry around extra,
     seemingly unnecessary information. However, this is actually exactly the
     information we need to reconstruct in φ(for ...)! However, we _know_ that
     this extra information is a *zero-change*, and our current scheme exposes
     this information to the optimizer (XXX DOES OUR OPTIMIZER USE IT?), while
     this scheme would not make it quite so obvious.

   - Second, letting ↓e be the intro for FA, what is δ↓e? Unlike {e}, it's not
     ⊥, because e *can* change. It's also not ↓δe; this doesn't typecheck,
     because ΔA ≠ A. Instead, the best I can think of is ↓(φe ⊕ δe), (which,
     incidentally, requires defining the ⊕ operator in Datafun). But then what
     is δ{e}? It's

         δ{e} = δ↓[e] = ↓(φ[e] ⊕ δ[e]) = ↓[φe,δe]

     which is annoying, because we _know_ it's safe to let δ{e} = ⊥. So this
     becomes an additional optimization, rather than a consequence of our translation.

   Neither of these are deal-breakers, but they are annoying.

* Syntax sugar / conveniences

sets          → downsets + discreteness
{e₁...eₙ}     → {[e₁]} ∨ ... ∨ {[eₙ]}
discrete case → splitsum
if-then-else  → case & isEmpty
true,false    → {[()]}, ⊥
when          → for & sets of units
(fix x.e)     → fix (box (λx.e))
e₁ = e₂       → box e₁ = box e₂

explain why booleans-as-sets-of-units is helpful
(ie. use "when")
